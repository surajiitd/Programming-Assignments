{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgWlyYejDwBF"
   },
   "source": [
    "# EECS 442 Assignment 5(3): Semantic Segmentaion\n",
    "In this part, you will design and implement your Convolutional Neural Networks to perform semantic segmentation on the Mini Facade dataset.\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E54t0DTZDzpl"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hello WORLD, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFgib63vLvfU"
   },
   "source": [
    "## Setup\n",
    "First, we will install some required packages for this notebook and download the Mini Facade dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDEtIQjnEWN5"
   },
   "outputs": [],
   "source": [
    "# install required packages and download the dataset\n",
    "!pip install colormap easydev pypng\n",
    "!wget http://web.eecs.umich.edu/~justincj/teaching/eecs442/resources/facades.zip\n",
    "!unzip facades.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szOGZTMpDXFa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import png\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from colormap.colors import Color, hex2rgb\n",
    "from sklearn.metrics import average_precision_score as ap_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZiuefySEJ6J"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU. You are good to go!\")\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    raise Exception(\"WARNING: Could not find GPU! Using CPU only. \\\n",
    "To enable GPU, please to go Edit > Notebook Settings > Hardware \\\n",
    "Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwWirxVjELJ2"
   },
   "source": [
    "## Dataset\n",
    "We will create a custom Dataset function for the Mini Facade dataset. You don't have to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK6si5Kzp7cb"
   },
   "outputs": [],
   "source": [
    "class FacadeDataset(Dataset):\n",
    "  def __init__(self, flag, dataDir='./facades/', data_range=(0, 8), n_class=5, \n",
    "               onehot=False):\n",
    "    self.onehot = onehot\n",
    "    assert(flag in ['train', 'eval', 'test', 'test_dev', 'kaggle'])\n",
    "    print(\"load \"+ flag+\" dataset start\")\n",
    "    print(\"    from: %s\" % dataDir)\n",
    "    print(\"    range: [%d, %d)\" % (data_range[0], data_range[1]))\n",
    "    self.dataset = []\n",
    "    for i in range(data_range[0], data_range[1]):\n",
    "      img = Image.open(os.path.join(dataDir,flag,'eecs442_%04d.jpg' % i))\n",
    "\n",
    "      pngreader = png.Reader(filename=os.path.join(dataDir,flag,\n",
    "                                                   'eecs442_%04d.png' % i))\n",
    "      w,h,row,info = pngreader.read()\n",
    "      label = np.array(list(row)).astype('uint8')\n",
    "\n",
    "      # Normalize input image\n",
    "      img = np.asarray(img).astype(\"f\").transpose(2, 0, 1)/128.0-1.0\n",
    "      # Convert to n_class-dimensional onehot matrix\n",
    "      label_ = np.asarray(label)\n",
    "      label = np.zeros((n_class, img.shape[1], img.shape[2])).astype(\"i\")\n",
    "      for j in range(n_class):\n",
    "          label[j, :] = label_ == j\n",
    "      self.dataset.append((img, label))\n",
    "    print(\"load dataset done\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    img, label = self.dataset[index]\n",
    "    label = torch.FloatTensor(label)\n",
    "    if not self.onehot:\n",
    "      label = torch.argmax(label, dim=0)\n",
    "    else:\n",
    "      label = label.long()\n",
    "\n",
    "    return torch.FloatTensor(img), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ompM1vf2UmzS"
   },
   "source": [
    "Now, we will create a dataloader for the Mini Facade dataset. You have to play with the size of train-val split and adjust the batch sizes for the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjLnwnbOD285"
   },
   "outputs": [],
   "source": [
    "N_CLASS=5\n",
    "##############################################################################\n",
    "# TODO: Change data_range so that train_data and val_data splits the 906     #\n",
    "# samples under \"train\" folder. You can decide how to split.                 #\n",
    "#                                                                            # \n",
    "# TODO: Adjust batch_size for loaders                                        #\n",
    "##############################################################################\n",
    "train_data = FacadeDataset(flag='train', data_range=(0,800), onehot=False)\n",
    "train_loader = DataLoader(train_data, batch_size=4)\n",
    "val_data = FacadeDataset(flag='train', data_range=(800,906), onehot=False)\n",
    "val_loader = DataLoader(val_data, batch_size=4)\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################\n",
    "test_data = FacadeDataset(flag='test_dev', data_range=(0,114), onehot=False)\n",
    "test_loader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "# ap_loader for calculating Average Precision\n",
    "ap_data = FacadeDataset(flag='test_dev', data_range=(0,114), onehot=True)\n",
    "ap_loader = DataLoader(ap_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fU5kNVZFIpg"
   },
   "source": [
    "## Model\n",
    "Design and implement your Convolutional NeuralNetworks to perform semantic segmentation on the Mini Facade dataset. \n",
    "\n",
    "You can build a simple neural network based on the U-net [1]:\n",
    "1. Conv-ReLU-Conv-ReLU that goes from `3 -> 64 -> 64` ( H x W )\n",
    "2. Maxpool (`nn.MaxPool2d`) to reduce the size of the feature map by half ( H/2 x W/2 )\n",
    "3. Conv-ReLU-Conv-ReLU that goes from `64 -> 128 -> 128` ( H/2 x W/2 )\n",
    "4. Maxpool (`nn.MaxPool2d`) to reduce the size of the feature map by half ( H/4 x W/4 )\n",
    "5. Conv that goes from `128 -> 128` ( H/4 x W/4 )\n",
    "6. Upsample (`nn.Upsample`) to increase of size of the feature map by two ( H/2 x W/2 )\n",
    "6. Conv-ReLU-Conv-ReLU that goes from `128 -> 64 -> 64` ( H/2 x W/2 )\n",
    "7. Upsample (`nn.Upsample`) to increase of size of the feature map by two ( H x W )\n",
    "8. Conv-ReLU-Conv-ReLU that goes from `64 -> 64 -> 5` ( H x W )\n",
    "\n",
    "Be careful while choosing the kernel size, padding and stride in the convolution layer. You can compute the output shape after a convolution by using the formula:\n",
    "\n",
    "$$\n",
    "X_{out} = (X_{in} - K + 2P)/S +1\n",
    "$$\n",
    "\n",
    "Where, $X_{out}$ & $X_{in}$ are the output and input shape of the feature map and $K$, $P$ & $S$ are the kernel size, padding and stride repectively.\n",
    "\n",
    "[1] O. Ronneberger, P. Fischer, and T. Brox, “U-net:  Convolutional networks for biomedical image seg-mentation,”ArXiv, vol. abs/1505.04597, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0lZ1PvTFJme"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.n_class = N_CLASS\n",
    "    ########################################################################\n",
    "    # TODO: Implement a sematic segmentation model                         #\n",
    "    ########################################################################\n",
    "    self.layers = nn.Sequential(\n",
    "\n",
    "        nn.Conv2d(3, self.n_class, 1, padding=0),\n",
    "        nn.ReLU(inplace=True),\n",
    "        )\n",
    "    ########################################################################\n",
    "    #                             END OF YOUR CODE                         #\n",
    "    ########################################################################\n",
    "\n",
    "  def forward(self, x):\n",
    "    ########################################################################\n",
    "    # TODO: Implement the forward pass                                     #\n",
    "    ########################################################################\n",
    "    x = self.layers(x)\n",
    "    ########################################################################\n",
    "    #                             END OF YOUR CODE                         #\n",
    "    ########################################################################\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyWQnSJ21Rxd"
   },
   "source": [
    "Ininitialize your model and look at the structure of your model, [torchsummary](https://github.com/sksq96/pytorch-summary) is a useful library to look at the computation graph of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssxWj9dA1SOb"
   },
   "outputs": [],
   "source": [
    "name = 'starter_net'\n",
    "net = Net().to(device)\n",
    "# visualizing the model\n",
    "print('Your network:')\n",
    "summary(net, (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOPAI1wYFQxc"
   },
   "source": [
    "Once you have completed the model implementation in the cell above, run the cell below to load helper functions and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUXCW18nFK6p"
   },
   "outputs": [],
   "source": [
    "def save_label(label, path):\n",
    "  '''\n",
    "  Function for ploting labels.\n",
    "  '''\n",
    "  colormap = [\n",
    "      '#000000',\n",
    "      '#0080FF',\n",
    "      '#80FF80',\n",
    "      '#FF8000',\n",
    "      '#FF0000',\n",
    "  ]\n",
    "  assert(np.max(label)<len(colormap))\n",
    "  colors = [hex2rgb(color, normalise=False) for color in colormap]\n",
    "  w = png.Writer(label.shape[1], label.shape[0], palette=colors, bitdepth=4)\n",
    "  with open(path, 'wb') as f:\n",
    "      w.write(f, label)\n",
    "\n",
    "def train(trainloader, net, criterion, optimizer, device, epoch):\n",
    "  '''\n",
    "  Function for training.\n",
    "  '''\n",
    "  start = time.time()\n",
    "  running_loss = 0.0\n",
    "  cnt = 0\n",
    "  net = net.train()\n",
    "  for images, labels in tqdm(trainloader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = net(images)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    cnt += 1\n",
    "  end = time.time()\n",
    "  running_loss /= cnt\n",
    "  print('\\n [epoch %d] loss: %.3f elapsed time %.3f' %\n",
    "        (epoch, running_loss, end-start))\n",
    "  return running_loss\n",
    "\n",
    "def test(testloader, net, criterion, device):\n",
    "  '''\n",
    "  Function for testing.\n",
    "  '''\n",
    "  losses = 0.\n",
    "  cnt = 0\n",
    "  with torch.no_grad():\n",
    "    net = net.eval()\n",
    "    for images, labels in tqdm(testloader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      output = net(images)\n",
    "      loss = criterion(output, labels)\n",
    "      losses += loss.item()\n",
    "      cnt += 1\n",
    "  print('\\n',losses / cnt)\n",
    "  return (losses/cnt)\n",
    "\n",
    "\n",
    "def cal_AP(testloader, net, criterion, device):\n",
    "  '''\n",
    "  Calculate Average Precision\n",
    "  '''\n",
    "  losses = 0.\n",
    "  cnt = 0\n",
    "  with torch.no_grad():\n",
    "    net = net.eval()\n",
    "    preds = [[] for _ in range(5)]\n",
    "    heatmaps = [[] for _ in range(5)]\n",
    "    for images, labels in tqdm(testloader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      output = net(images).cpu().numpy()\n",
    "      for c in range(5):\n",
    "        preds[c].append(output[:, c].reshape(-1))\n",
    "        heatmaps[c].append(labels[:, c].cpu().numpy().reshape(-1))\n",
    "\n",
    "    aps = []\n",
    "    for c in range(5):\n",
    "      preds[c] = np.concatenate(preds[c])\n",
    "      heatmaps[c] = np.concatenate(heatmaps[c])\n",
    "      if heatmaps[c].max() == 0:\n",
    "        ap = float('nan')\n",
    "      else:\n",
    "        ap = ap_score(heatmaps[c], preds[c])\n",
    "        aps.append(ap)\n",
    "      print(\"AP = {}\".format(ap))\n",
    "    print(\"Average Precision (all classes) = {}\".format(np.mean(aps)))\n",
    "  return None\n",
    "\n",
    "\n",
    "def get_result(testloader, net, device, folder='output_train'):\n",
    "  result = []\n",
    "  cnt = 1\n",
    "  os.makedirs(folder, exist_ok=True)\n",
    "  with torch.no_grad():\n",
    "    net = net.eval()\n",
    "    cnt = 0\n",
    "    for images, labels in tqdm(testloader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      output = net(images)[0].cpu().numpy()\n",
    "      c, h, w = output.shape\n",
    "      assert(c == N_CLASS)\n",
    "      y = np.argmax(output, 0).astype('uint8')\n",
    "      gt = labels.cpu().data.numpy().squeeze(0).astype('uint8')\n",
    "      save_label(y, './{}/y{}.png'.format(folder, cnt))\n",
    "      save_label(gt, './{}/gt{}.png'.format(folder, cnt))\n",
    "      plt.imsave('./{}/x{}.png'.format(folder, cnt),\n",
    "                 ((images[0].cpu().data.numpy()+1)*128).astype(np.uint8).transpose(1,2,0))\n",
    "      cnt += 1\n",
    "\n",
    "def plot_hist(trn_hist, val_hist):\n",
    "    x = np.arange(len(trn_hist))\n",
    "    plt.figure()\n",
    "    plt.plot(x, trn_hist)\n",
    "    plt.plot(x, val_hist)\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.xticks(x)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dKfZ2SnWnut"
   },
   "source": [
    "Experiment with with different optimizers, parameters (such as learning rate) and number of epochs. We expect you to achieve **0.45 AP** on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rh5n-zKMFViG"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "##############################################################################\n",
    "# TODO: Modify the lines below to experiment with different optimizers,      #\n",
    "# parameters (such as learning rate) and number of epochs.                   #\n",
    "##############################################################################\n",
    "optimizer = torch.optim.Adam(net.parameters(), 5e-4, weight_decay=1e-5)\n",
    "num_epoch = 10\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "\n",
    "print('\\nStart training')\n",
    "trn_hist = []\n",
    "val_hist = []\n",
    "for epoch in range(num_epoch): #TODO: Change the number of epochs\n",
    "  print('-----------------Epoch = %d-----------------' % (epoch+1))\n",
    "  trn_loss = train(train_loader, net, criterion, optimizer, device, epoch+1)\n",
    "  print('Validation loss: ')\n",
    "  val_loss = test(val_loader, net, criterion, device)\n",
    "  trn_hist.append(trn_loss)\n",
    "  val_hist.append(val_loss)\n",
    "\n",
    "plot_hist(trn_hist, val_hist)\n",
    "##########################################################################\n",
    "# TODO: Submit the \"Average Precision(all classes)\" value in the report  #\n",
    "##########################################################################\n",
    "print('\\nFinished Training, Testing on test set')\n",
    "test(test_loader, net, criterion, device)\n",
    "print('\\nGenerating Unlabeled Result')\n",
    "##############################################################################\n",
    "# You can visualize your segmentation results using get_results function     #\n",
    "# Your result will be dumped in the folder 'output_test'.                    #\n",
    "# There will be three files for each image:                                   # \n",
    "#   (1) gt<num>.png (ground truth label)                                     #\n",
    "#   (2) x<num>.png (input RGB image)                                         #\n",
    "#   (3) y<num>.png (predicted output)                                        #\n",
    "#                                                                            #\n",
    "# TODO: Find a satisfactory result by running the next cell and report       #\n",
    "#       the plot of gt<num>.png, x<num>.png and y<num>.png in the pdf        #\n",
    "# Note: Your submission doesn't have to be perfect.                          #\n",
    "##############################################################################\n",
    "result = get_result(test_loader, net, device, folder='output_test')\n",
    "\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "torch.save(net.state_dict(), './models/model_{}.pth'.format(name))\n",
    "\n",
    "cal_AP(ap_loader, net, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwYgDofP6bh9"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: Find a satisfactory result from the 114 test images            #\n",
    "#       Submit the result in the pdf                                   #\n",
    "# Note: Your submission doesn't have to be perfect.                    #\n",
    "########################################################################\n",
    "img_idx = 0\n",
    "########################################################################\n",
    "#                             END OF YOUR CODE                         #\n",
    "########################################################################\n",
    "output_dir = 'output_test'\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(Image.open(os.path.join(output_dir, 'x{}.png'.format(img_idx))))\n",
    "plt.title('Input Image', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(Image.open(os.path.join(output_dir, 'gt{}.png'.format(img_idx))))\n",
    "plt.title('Ground Truth Segmentation Map', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(Image.open(os.path.join(output_dir, 'y{}.png'.format(img_idx))))\n",
    "plt.title('Predicted Segmentation Map', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.gcf().set_size_inches(18, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Bz6Ic8f1m-"
   },
   "source": [
    "## Acknowledgement\n",
    "The Mini Facade dataset are modified from CMP Facade Database by $Radim \\ Tyle \\check{c}ek$ and $Radim \\ \\check{S}a^{'}ra$. Please feel free to similarly re-use our problems while similarly crediting us."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMpn2QZXvz37/WFU6F1Qk/Y",
   "collapsed_sections": [],
   "name": "part3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
