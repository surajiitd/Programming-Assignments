{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQQwn7SH7w0r"
   },
   "source": [
    "# EECS 442 Assignment 5(2) - PyTorch ConvNets, Saliency Maps and Adversarial Attack\n",
    "In this notebook we will explore how to use a pre-trained PyTorch convolution neural network (ConvNet), generate a saliency map and then fool the same network by using gradient ascent.\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eFPFu-q83RT"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hello WORLD, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avzyZRQ4HXEQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGyzNtrV9Gtt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "SQUEEZENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float)\n",
    "SQUEEZENET_STD = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float)\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW4zzv00YcT6"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU. You are good to go!\")\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    raise Exception(\"WARNING: Could not find GPU! Using CPU only. \\\n",
    "To enable GPU, please to go Edit > Notebook Settings > Hardware \\\n",
    "Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6V0Z5Lw_cuo"
   },
   "source": [
    "For all of our experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet [1]. We can use any model here, but for the purposes of this assignment we will use SqueezeNet [2], which achieves accuracies comparable to AlexNet but with a significantly reduced parameter count and computational complexity.\n",
    "\n",
    "Using SqueezeNet rather than AlexNet or VGG or ResNet means that we can easily perform all experiments without heavy computation. Run the following cell to download and initialize your model.\n",
    "\n",
    "[1] Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015\n",
    "\n",
    "[2] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwcF2163_d9X"
   },
   "outputs": [],
   "source": [
    "print('Download and load the pretrained SqueezeNet model.')\n",
    "model = torchvision.models.squeezenet1_1(pretrained=True).to(device)\n",
    "\n",
    "# We don't want to train the model, so tell PyTorch not to compute gradients\n",
    "# with respect to model parameters.\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "    \n",
    "# Make sure the model is in \"eval\" mode\n",
    "model.eval()\n",
    "\n",
    "# you may see warning regarding initialization deprecated, that's fine, \n",
    "# please continue to next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meTruYDBGSLY"
   },
   "source": [
    "Next, we will download the imagenet labels which we are going to use in the notebook. ImageNet labels are stored in the `idx2label` dictionary of `{index(int): label(str)}`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHL95LvKEhFx"
   },
   "outputs": [],
   "source": [
    "# Loading the imagenet class labels\n",
    "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
    "\n",
    "class_idx = json.load(open(\"imagenet_class_index.json\"))\n",
    "idx2label = {k:class_idx[str(k)][1] for k in range(len(class_idx))}\n",
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAGOycoQ_X8x"
   },
   "source": [
    "### Helper Functions\n",
    "\n",
    "Our pretrained model was trained on images that had been preprocessed by subtracting the per-color mean and dividing by the per-color standard deviation. We define a few helper functions for performing and undoing this preprocessing. You don't need to do anything in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgrlY4_U_Z1S"
   },
   "outputs": [],
   "source": [
    "def preprocess(img, size=224):\n",
    "  transform = T.Compose([\n",
    "    T.Resize((size, size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "          std=SQUEEZENET_STD.tolist()),\n",
    "    T.Lambda(lambda x: x[None]),\n",
    "  ])\n",
    "  return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "  transform = T.Compose([\n",
    "    T.Lambda(lambda x: x[0]),\n",
    "    T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
    "    T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "    T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "    T.ToPILImage(),\n",
    "  ])\n",
    "  return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "  low, high = x.min(), x.max()\n",
    "  x_rescaled = (x - low) / (high - low)\n",
    "  return x_rescaled\n",
    "  \n",
    "def blur_image(X, sigma=1):\n",
    "  X_np = X.cpu().clone().numpy()\n",
    "  X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "  X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "  X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrrlCIufBt3e"
   },
   "source": [
    "## Task 1 - Pre-trained Convolution Network\n",
    "In order to get a better sense of the classification decisions made by convolutional networks, your job is now to experiment by running whatever images you want through a model pretrained on ImageNet. These can be images from your own photo collection, from the internet, or somewhere else but they **should belong to one of the ImageNet classes**. Look at the `idx2label` dictionary for all the ImageNetclasses.\n",
    "\n",
    "You need to find:\n",
    "1. One image (`img1`) where the SqueezeNet model gives reasonable predictions, and produces a category label that seems to correctly describe the content of the image\n",
    "2. One image (`img2`) where the SqueezeNet model gives unreasonable predictions, and produces a category label that does not correctly describe the content of the image.\n",
    "\n",
    "You can upload images in Colab by using the upload button on the top left. For more details about using Colab, please see our [Colab tutorial](https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/colab.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHuRe4VXBtf0"
   },
   "outputs": [],
   "source": [
    "}###############################################################################\n",
    "# TODO: Upload your image and run the forward pass to get the ImageNet class. #\n",
    "# This code will crash when you run it, since the maxresdefault.jpg image is  #\n",
    "# not found. You should upload your own images to the Colab notebook and edit #\n",
    "# these lines to load your own image.                                         #\n",
    "###############################################################################\n",
    "img1 = Image.open('maxresdefault.jpg')\n",
    "img2 = Image.open('maxresdefault.jpg')\n",
    "##############################################################################\n",
    "#               END OF YOUR CODE                                             #\n",
    "##############################################################################\n",
    "for img in [img1, img2]:\n",
    "  X = preprocess(img).to(device)\n",
    "  pred_class = torch.argmax(model(X)).item()\n",
    "  plt.imshow(img)\n",
    "  plt.title('Predicted Class: %s' % idx2label[pred_class])\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCjaqoDYpk1b"
   },
   "source": [
    "## Task 2 - Saliency Maps\n",
    "Using this pretrained model, we will compute class saliency maps as described in Section 3.1 of [2].\n",
    "\n",
    "A **saliency map** tells us the degree to which each pixel in the image affects the classification score for that image. To compute it, we compute the gradient of the unnormalized score corresponding to the correct class (which is a scalar) with respect to the pixels of the image. If the image has shape `(3, H, W)` then this gradient will also have shape `(3, H, W)`; for each pixel in the image, this gradient tells us the amount by which the classification score will change if the pixel changes by a small amount. To compute the saliency map, we take the absolute value of this gradient, then take the maximum value over the 3 input channels; the final saliency map thus has shape `(H, W)` and all entries are nonnegative.\n",
    "\n",
    "[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n",
    "Image Classification Models and Saliency Maps\", ICLR Workshop 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfRZF8nAplj7"
   },
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "  \"\"\"\n",
    "  Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "  Input:\n",
    "  - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "  - y: Labels for X; LongTensor of shape (N,)\n",
    "  - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "  Returns:\n",
    "  - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "  images.\n",
    "  \"\"\"\n",
    "  # Make input tensor require gradient\n",
    "  X.requires_grad_()\n",
    "  \n",
    "  saliency = None\n",
    "  ##############################################################################\n",
    "  # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "  # the model to compute the gradient of the correct class score with respect  #\n",
    "  # to each input image. You first want to compute the loss over the correct    #\n",
    "  # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "  # the gradients with a backward pass.                                        #\n",
    "  ##############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  pass\n",
    "  ##############################################################################\n",
    "  #               END OF YOUR CODE                                             #\n",
    "  ##############################################################################\n",
    "  return saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THkj_wW-p-EW"
   },
   "source": [
    "Once you have completed the implementation in the cell above, run the following to visualize the saliency map for the two images from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqOjop_Qp84M"
   },
   "outputs": [],
   "source": [
    "def show_saliency_maps(X_tensor, y_tensor):\n",
    "  # Compute saliency maps for images in X\n",
    "  saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "\n",
    "  # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "  # and saliency maps together.\n",
    "  saliency = saliency.to('cpu').numpy()\n",
    "  N = X.shape[0]\n",
    "  for i in range(N):\n",
    "    plt.subplot(2, N, i + 1)\n",
    "    plt.imshow(deprocess(X_tensor[i].cpu().unsqueeze(0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(idx2label[y_tensor[i].item()])\n",
    "    plt.subplot(2, N, N + i + 1)\n",
    "    plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(12, 8)\n",
    "  plt.show()\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Visualize the saliency map for the two images from the previous part #\n",
    "##############################################################################\n",
    "img1 = Image.open('maxresdefault.jpg') \n",
    "img2 = Image.open('maxresdefault.jpg') \n",
    "##############################################################################\n",
    "#               END OF YOUR CODE                                             #\n",
    "##############################################################################\n",
    "X = torch.cat([preprocess(x) for x in [img1, img2]], dim=0).to(device)\n",
    "y = torch.argmax(model(X), dim=1)\n",
    "show_saliency_maps(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFrsnEnw-zTJ"
   },
   "source": [
    "## Task 3: Adversarial Attack \n",
    "We can use image gradients to generate \"adversarial attacks\" as discussed in [3].\n",
    "Given an image and a target class, we can perform gradient descent over the image to maximize the target class, stopping when the network classifies the image as the target class.\n",
    "Implement the following function to generate adversarial attacks.\n",
    "\n",
    "[3] Szegedy et al, \"Intriguing properties of neural networks\", ICLR 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvMb6kwW-z60"
   },
   "outputs": [],
   "source": [
    "def make_adversarial_attack(X, target_y, model):\n",
    "  \"\"\"\n",
    "  Generate an adversarial attack that is close to X, but that the model classifies\n",
    "  as target_y.\n",
    "\n",
    "  Inputs:\n",
    "  - X: Input image; Tensor of shape (1, 3, 224, 224)\n",
    "  - target_y: An integer in the range [0, 1000)\n",
    "  - model: A pretrained CNN\n",
    "\n",
    "  Returns:\n",
    "  - X_adv: An image that is close to X, but that is classifed as target_y\n",
    "  by the model.\n",
    "  \"\"\"\n",
    "  # Initialize our adversarial attack to the input image, and make it require gradient\n",
    "  X_adv = X.clone()\n",
    "  X_adv = X_adv.requires_grad_()\n",
    "  \n",
    "  learning_rate = 1\n",
    "  ##############################################################################\n",
    "  # TODO: Generate an adversarial attack X_adv that the model will classify    #\n",
    "  # as the class target_y. You should perform gradient ascent on the score     #\n",
    "  # of the target class, stopping when the model is fooled.                    #\n",
    "  # When computing an update step, first normalize the gradient:                #\n",
    "  #   dX = learning_rate * g / ||g||_2                                         #\n",
    "  #                                                                            #\n",
    "  # You should write a training loop.                                          #\n",
    "  #                                                                            #\n",
    "  # HINT: For most examples, you should be able to generate an adversarial     #\n",
    "  # attack in fewer than 100 iterations of gradient ascent.                    #\n",
    "  # You can print your progress over iterations to check your algorithm.       #\n",
    "  ##############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  pass\n",
    "  ##############################################################################\n",
    "  #                             END OF YOUR CODE                               #\n",
    "  ##############################################################################\n",
    "  return X_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ojiTnUN-5Gh"
   },
   "source": [
    "Run the following cell to generate an adversarial attack. You should ideally see at first glance no major difference between the original and attacked images, and the network should now make an incorrect prediction on the attacked one. However you should see a bit of random noise if you look at the 10x magnified difference between the original and attacked images. \n",
    "\n",
    "Now, use the image from `task 1` which gave the **correct** prediction to fool the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RORZAdjI-5eI"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Use the correctly predicted image from task 1 to fool the network    #\n",
    "# Choose a class (target_y) from idx2label dictionary to fool the nwtwork    #\n",
    "##############################################################################\n",
    "img = Image.open('maxresdefault.jpg')\n",
    "target_y = 6\n",
    "##############################################################################\n",
    "#               END OF YOUR CODE                                             #\n",
    "##############################################################################\n",
    "X_tensor = preprocess(img).to(device)\n",
    "\n",
    "\n",
    "print('Print your progress using the following format: \\\n",
    "the model is fooled if the target score and max score are the same.')\n",
    "print('Iteration %d: target score %.3f, max score %.3f')\n",
    "X_adv = make_adversarial_attack(X_tensor, target_y, model)\n",
    "\n",
    "scores = model(X_adv)\n",
    "assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLhfex91--ta"
   },
   "source": [
    "After generating an adversarially attacked image, run the following cell to visualize the original image, the attacked image, as well as the difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vdj1eJs-9sP"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Submit the plot                                                      #\n",
    "##############################################################################\n",
    "gt_class = idx2label[torch.argmax(model(X_tensor)).item()]\n",
    "X_adv = X_adv.to('cpu')\n",
    "X_adv_np = deprocess(X_adv.clone())\n",
    "X_adv_np = np.asarray(X_adv_np).astype(np.uint8)\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img)\n",
    "plt.title(gt_class)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_adv_np)\n",
    "plt.title(idx2label[target_y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "X_pre = preprocess(img)\n",
    "diff = np.asarray(deprocess(X_adv - X_pre, should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Difference')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "diff = np.asarray(deprocess(10 * (X_adv - X_pre), should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Magnified difference (10x)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.gcf().set_size_inches(18, 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "part2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
